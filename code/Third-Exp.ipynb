{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import multiprocessing\n",
    "import random\n",
    "import functools\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from mpl_toolkits import mplot3d\n",
    "import seaborn as sns\n",
    "\n",
    "from pandas import plotting\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "\n",
    "from trackml.dataset import load_event, load_dataset\n",
    "from trackml.randomize import shuffle_hits\n",
    "from trackml.score import score_event\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dataset_filenames_from_dir(path_to_datasets_dir):\n",
    "    dataset_filenames = os.listdir(path_to_datasets_dir)\n",
    "    event_filenames = {}\n",
    "    for filename in dataset_filenames:\n",
    "        path_to_file = os.path.join(path_to_datasets_dir, filename)\n",
    "        event_id = filename[5:14]\n",
    "        if event_id not in event_filenames:\n",
    "            event_filenames[event_id] = [path_to_file]\n",
    "        else:\n",
    "            event_filenames[event_id].append(path_to_file)\n",
    "    return event_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_events(indexes_list, event_names):\n",
    "    return tuple(event_names[i] for i in indexes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_events_select(event_names, subset_size):\n",
    "    event_names_len = len(event_names)\n",
    "    indexes = select_random_indexses_subset(event_names_len, subset_size)\n",
    "    return select_events(indexes, event_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def offset_events_select(event_names, subset_size, offset):\n",
    "    event_names_len = len(event_names)\n",
    "    indexes = select_offset_indexses_subset(event_names_len, subset_size, offset)\n",
    "    return select_events(indexes, event_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_random_indexses_subset(size, subset_size):\n",
    "    return random.sample(tuple(range(size)), subset_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_random_indexses_subset(size, subset_size):\n",
    "    return random.sample(tuple(range(size)), subset_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_offset_indexses_subset(size, subset_size, offset):\n",
    "    return tuple(range(size))[offset:offset + subset_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_events(event_ids, event_dir):\n",
    "    events = {}\n",
    "    for event_id in event_ids:\n",
    "        hits, cells, particles, truth = load_event('../input/{}/event{}'.format(event_dir, event_id))\n",
    "        events[event_id] = {\n",
    "            'hits': hits,\n",
    "            'cells': cells,\n",
    "            'particles': particles,\n",
    "            'truth': truth\n",
    "        }\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_particles_with_zero_id(particles):\n",
    "    return particles.loc[particles['particle_id'] != 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_particles_with_zero_hits(particles):\n",
    "    return particles.loc[particles['nhits'] != 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_zero_particles(particles):\n",
    "    #return particles.loc[(particles['particle_id'] != 0) and (particles['nhits'] != 0), :]\n",
    "    not_zero_id_particles = remove_particles_with_zero_id(particles)\n",
    "    #return remove_particles_with_zero_hits(not_zero_id_particles)\n",
    "    return not_zero_id_particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_event(event):\n",
    "    cleaned_event = {}\n",
    "    cleaned_event['particles'] = remove_zero_particles(event['particles'])\n",
    "    cleaned_event['hits'] = event['hits']\n",
    "    cleaned_event['cells'] = event['cells']\n",
    "    cleaned_event['truth'] = event['truth']\n",
    "    return cleaned_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_events(events):\n",
    "    cleaned_events = {}\n",
    "    return {event_id: clean_event(event) for event_id, event in events.items()}        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_particle_truth(particle_id, truth):\n",
    "    return truth[truth['particle_id'] == particle_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_particle_truth_hits_id(particle_truth):\n",
    "    return particle_truth['hit_id'].tolist()\n",
    "    #return particle_truth['hit_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_particle_hits(particle_hit_ids, hits):\n",
    "    #return hits[hits['hit_id'].reset_index(drop=True) == particle_hit_ids.reset_index(drop=True)]\n",
    "    return hits[hits['hit_id'].isin(particle_hit_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_particle_truth_path(particle_truth):\n",
    "    return tuple([tuple(point[1]) for point in particle_truth.loc[:, ('tx', 'ty', 'tz')].iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_particle_truth_momentums(particle_truth):\n",
    "    return particle_truth.loc[:, ('tpx', 'tpy', 'tpz')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_particle_hits_path(particle_hits):\n",
    "    return particle_hits.loc[:, ('x', 'y', 'z')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_particle_path_length(particle_path):\n",
    "    path_as_vectors = (\n",
    "        end_point - begin_point for end_point, begin_point in zip(\n",
    "        particle_path[1:].values, particle_path[:-1].values\n",
    "        )\n",
    "    )\n",
    "    return sum(math.sqrt(np.dot(vector, vector)) for vector in path_as_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_point_closenes(first_point, second_point, eps=0.01):\n",
    "    return all(\n",
    "        [\n",
    "            abs( abs(first_point_coord / second_point_coord) - 1) <= eps \\\n",
    "            for first_point_coord, second_point_coord  in zip(first_point, second_point)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_point_occurences(point, df, point_columns, eps=0.01):\n",
    "    coordinates_df = df[point_columns]\n",
    "    point_occurences = []\n",
    "    for i, coordinates in coordinates_df.iterrows():\n",
    "        if check_point_closenes(point, coordinates, eps=eps):\n",
    "            point_occurences.append((i, coordinates))\n",
    "    return point_occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_first_point_occurence(point, df, point_columns, eps=0.01):\n",
    "    coordinates_df = df[point_columns]\n",
    "    point_occurence = None\n",
    "    for i, coordinates in coordinates_df.iterrows():\n",
    "        if check_point_closenes(point, coordinates, eps=eps):\n",
    "            point_occurence = (i, coordinates)\n",
    "            break;\n",
    "    return point_occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_all_event_truth_patches(event_truth, batch_size):\n",
    "    all_event_truth_patches = []\n",
    "    unique_partile_ids = set(event_truth['particle_id'].tolist())\n",
    "    total_patches_num  = len(unique_partile_ids)\n",
    "    for particle_id in unique_partile_ids:\n",
    "        number_current_added_pathes = batch_size * len(all_event_truth_patches)\n",
    "        non_proceed_patches_num = total_patches_num - number_current_added_pathes\n",
    "        if non_proceed_patches_num >= batch_size:\n",
    "            iterations_num = batch_size \n",
    "        else:\n",
    "            iterations_num = non_proceed_patches_num\n",
    "        batch = []\n",
    "        for i in range(iterations_num):\n",
    "            particle_truth = extract_particle_truth(particle_id, event_truth)\n",
    "            batch.append(get_particle_truth_path(particle_truth))\n",
    "        all_event_truth_patches.append(tuple(batch))\n",
    "    return tuple(all_event_truth_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_all_event_truth_patches(event_truth):\n",
    "    all_event_truth_patches = []\n",
    "    unique_particle_ids = set(event_truth['particle_id'].tolist())\n",
    "    for particle_id in unique_particle_ids:\n",
    "        particle_truth = extract_particle_truth(particle_id, event_truth)\n",
    "        all_event_truth_patches.append(get_particle_truth_path(particle_truth))\n",
    "    return tuple(all_event_truth_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_all_event_truth_patches(event_truth):\n",
    "    unique_particle_ids = set(event_truth['particle_id'].tolist())\n",
    "    return (\n",
    "        get_particle_truth_path(\n",
    "            extract_particle_truth(particle_id, event_truth)\n",
    "        ) for particle_id in unique_particle_ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_all_event_truth_patches(event_truth):\n",
    "    unique_particle_ids = set(event_truth['particle_id'].tolist())\n",
    "    return tuple([\n",
    "        get_particle_truth_path(\n",
    "            extract_particle_truth(particle_id, event_truth)\n",
    "        ) for particle_id in unique_particle_ids\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_closenest_path(truth_path, event_hits):\n",
    "    closenest_path = []\n",
    "    for point in truth_path:\n",
    "        point_occurence = find_first_point_occurence(point, event_hits, ['x', 'y', 'z'], eps=0.1)\n",
    "        if point_occurence:\n",
    "            closenest_path.append(point_occurence[1])\n",
    "    return closenest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_all_closenest_patches(truth_patches, event_hits):\n",
    "    print(\"enter in find_all_closenest_patches\")\n",
    "    all_closenest_patches = []\n",
    "    start_time = time.time()\n",
    "    for truth_path in truth_patches:\n",
    "        all_closenest_patches.append(find_closenest_path(truth_path, event_hits))\n",
    "    print(\"closenest pathces found or not found\")\n",
    "    print(\"elapsed time: {}\".format(time.time() - start_time))\n",
    "    return all_closenest_pathes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_closenest_patches_detector(event_hits):\n",
    "    def process_pathes_chunk(truth_pathes_chunk):\n",
    "        print(\"enter in proecess_pathes_chunk\")\n",
    "        all_closenest_patches = []\n",
    "        for truth_path in truth_patches_chunk:\n",
    "            all_closenest_patches.append(find_closenest_path(truth_path, event_hits))\n",
    "        print(\"process_patches_chunk: patches proceed\")\n",
    "        return all_closenest_patches\n",
    "    return process_pathes_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dummy_count_patches_num(patches):\n",
    "    i = 0\n",
    "    print(\"dummy_count, i before\", i)\n",
    "    for path in patches:\n",
    "        i += 1\n",
    "    print(\"dummy_count, i after\", i)\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_pathces_in_chunks(patches_list, num_of_chunks):\n",
    "    chunk_size = len(patches_list) // num_of_chunks\n",
    "    return tuple([patches_list[i * chunk_size:(i + 1) * chunk_size] for i in range(num_of_chunks)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event_filenames = read_dataset_filenames_from_dir('../input/train_1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event_ids = offset_events_select(list(event_filenames.keys()), 4, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('000001488', '000002134', '000001343', '000002668')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events = load_events(event_ids, 'train_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_events = clean_events(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event0 = list(cleaned_events.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event0_data = event0[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event0_truth = event0_data['truth']\n",
    "event0_hits = event0_data['hits']\n",
    "event0_particles = event0_data['particles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event0_all_pathes = extract_all_event_truth_patches(event0_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(event0_all_pathes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event1 = list(cleaned_events.items())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event1_data = event1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event1_truth = event1_data['truth']\n",
    "event1_hits = event1_data['hits']\n",
    "event1_particles = event1_data['particles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_closenest_patches = find_all_closenest_patches(event0_all_pathes, event1_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool_size = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "event0_patches_divided_to_chunks = divide_pathces_in_chunks(event0_all_pathes, pool_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy_count, i before 0\n",
      "dummy_count, i before 0\n",
      "dummy_count, i after 2033\n",
      "dummy_count, i after 2033\n",
      "dummy_count, i before 0\n",
      "dummy_count, i after 2033\n",
      "dummy_count, i before 0\n",
      "dummy_count, i after 2033\n"
     ]
    }
   ],
   "source": [
    "pool = multiprocessing.Pool(processes=pool_size)\n",
    "pool_outputs = pool.map(dummy_count_patches_num, event0_patches_divided_to_chunks)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2033, 2033, 2033, 2033]\n"
     ]
    }
   ],
   "source": [
    "print(pool_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#closenest_patches_detector = create_closenest_patches_detector(event1_hits)\n",
    "closenest_patches_detector = functools.partial(find_all_closenest_patches, event_hits=event1_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool(processes=pool_size)\n",
    "pool_outputs = pool.map(closenest_patches_detector, event0_patches_divided_to_chunks)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
